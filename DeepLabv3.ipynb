{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH-31nb2nolF",
        "outputId": "91ec49d3-cd3e-4d3c-9e21-bce3645f0a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pytorch/vision.git\n",
        "import sys\n",
        "sys.path.append('/content/vision/')\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i9-8j-Vnp9n",
        "outputId": "54e5ede3-e43e-42c1-e865-b9fe0f704798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 370557, done.\u001b[K\n",
            "remote: Counting objects: 100% (20797/20797), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1332/1332), done.\u001b[K\n",
            "remote: Total 370557 (delta 19527), reused 20577 (delta 19375), pack-reused 349760\u001b[K\n",
            "Receiving objects: 100% (370557/370557), 740.85 MiB | 47.89 MiB/s, done.\n",
            "Resolving deltas: 100% (342056/342056), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.segmentation.deeplabv3 import DeepLabV3_ResNet101_Weights\n",
        "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
        "import torchvision.models.segmentation as segmentation\n",
        "import torch.nn as nn\n",
        "\n",
        "weights = DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1\n",
        "model = segmentation.deeplabv3_resnet101(pretrained=False, progress=True, num_classes=21, aux_loss=None)\n",
        "state_dict = weights.get_state_dict(progress=True)\n",
        "in_channels = model.classifier[4].in_channels\n",
        "model.classifier[4] = nn.Conv2d(in_channels, 26, kernel_size=1)\n",
        "num_classes = model.classifier[4].out_channels\n",
        "\n",
        "# print(model)\n",
        "print(num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO8oseCPn_Lq",
        "outputId": "86332fc7-f74e-4e54-a0fd-c4ce1c573beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:00<00:00, 258MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet101_coco-586e9e4e.pth\n",
            "100%|██████████| 233M/233M [00:00<00:00, 261MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import random\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Pad((0, 396)),\n",
        "    transforms.Resize((513, 513)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.45969197, 0.44145255, 0.45046159],\n",
        "                         std=[0.27247444, 0.27013482, 0.27866579])\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(root=\"/content/drive/MyDrive/2DSSs\", transform=transform)\n",
        "indices = random.sample(range(len(dataset)), 4)\n",
        "subset_dataset = Subset(dataset, indices)\n",
        "dataloader = DataLoader(subset_dataset, batch_size=4)\n",
        "images, labels = next(iter(dataloader))\n",
        "print(f'images shape: {images.shape}')\n",
        "\n",
        "model.train()\n",
        "output_train = model(images)\n",
        "print(type(output_train))\n",
        "output_shape = output_train['out'].shape\n",
        "print(f'train mode output shape: {output_shape}')\n",
        "\n",
        "model.eval()\n",
        "output_eval = model(images)\n",
        "print(type(output_eval))\n",
        "output_shape = output_eval['out'].shape\n",
        "print(f'eval mode output shape: {output_shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxbTuyRSufaX",
        "outputId": "4e6f924e-98ee-4e7d-b56a-e4285c224fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images shape: torch.Size([4, 3, 513, 513])\n",
            "<class 'collections.OrderedDict'>\n",
            "train mode output shape: torch.Size([4, 26, 513, 513])\n",
            "<class 'collections.OrderedDict'>\n",
            "eval mode output shape: torch.Size([4, 26, 513, 513])\n"
          ]
        }
      ]
    }
  ]
}